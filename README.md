
# Fairness

## Books ##

* Online book: Fairness and machine learning, Limitations and Opportunities, Solon Barocas, Moritz Hardt, Arvind Narayanan https://fairmlbook.org/

## COMPAS case study##

* ProPublica https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing 
* [Todo]

## Demo's /tools ##
* https://github.com/google/ml-fairness-gym

## User experiments ##
* https://papers.ssrn.com/sol3/Papers.cfm?abstract_id=3503603

## NLP papers ##

### Embeddings
* Semantics derived automatically from language corpora contain human-like biases, Caliskan et al. Science 2017 https://science.sciencemag.org/content/356/6334/183
* Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings, Bolukbasi et al. NeurIPS 2016 https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings

### Other
* The Risk of Racial Bias in Hate Speech Detection, Sap et al., ACL 2019 https://www.aclweb.org/anthology/P19-1163/
* Google response gender bias in Google Translate https://www.blog.google/products/translate/reducing-gender-bias-google-translate/
* Co-reference resolution: Gender Bias in Coreference Resolution, Rudinger et al. NAACL 2018 https://www.aclweb.org/anthology/N18-2002/  and Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods Zhao et al. NAACL 2018 https://www.aclweb.org/anthology/N18-2003/ 
* [todo]

## Computer vision

* Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification, Buolamwini and Gebru, Proceedings of Machine Learning Research, http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf 2018

## Advertisements
* Discrimination in Online Ad Delivery, Sweeney, ACM Queue 2013 https://queue.acm.org/detail.cfm?id=2460278


# Interpretability / explainability

## Books

* Interpretable machine learning, Molnar https://christophm.github.io/interpretable-ml-book/ 

## General readings

* The Mythos of Model Interpretability, Lipton 2018 https://queue.acm.org/detail.cfm?id=3241340
* Guidotti et al. A Survey of Methods for Explaining Black Box Models, ACM Computing Surveys (CSUR) 2019 https://dl.acm.org/citation.cfm?id=3236009
* Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead, Rudin, Nature Machine Intelligence 2019 https://www.nature.com/articles/s42256-019-0048-x
* Towards A Rigorous Science of Interpretable Machine Learning,  Doshi-Velez and Kim 2017, https://arxiv.org/abs/1702.08608

## Local explanations

* “Why Should I Trust You?” Explaining the Predictions of Any Classifier, 
Ribeiro et al. KDD 2016 https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf
* A Unified Approach to Interpreting Model Predictions, Lundberg and Lee, NeurIPS 2017 http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf
* Understanding Black-box Predictions via Influence Functions, Koh and Liang, ICML 2017 http://proceedings.mlr.press/v70/koh17a/koh17a.pdf
* Layer-Wise Relevance Propagation: An Overview, Montavon et al. Explainable AI: Interpreting, Explaining and Visualizing Deep Learning  2019
https://link.springer.com/chapter/10.1007/978-3-030-28954-6_10
* Beyond saliency: understanding convolutional neural networks from saliency prediction on layer-wise relevance propagation, Heyi Li, Yunke Tian, Klaus Mueller, and Xin Chen. Image and Vision Computing 83 (2019). https://www.sciencedirect.com/science/article/pii/S0262885619300149
* [Todo] Gradient based methods

## Explainability and Bias in Multimodal Affective Computing 
* Escalante, Hugo Jair, et al. "Modeling, Recognizing, and Explaining Apparent Personality from Videos." IEEE Transactions on Affective Computing (2020), https://ieeexplore.ieee.org/abstract/document/8999746

## Evaluation

* Evaluating the visualization of what a deep neural network has learned, Samek et al.  IEEE Transactions on Neural Networks and Learning Systems 2017 https://ieeexplore.ieee.org/document/7552539 
* Towards Faithfully Interpretable NLP Systems:How Should We Define and Evaluate Faithfulness? https://www.aclweb.org/anthology/2020.acl-main.386.pdf
* [Todo] User experiments, trust..

## Visualization

* [todo]


## Toolkits
* AllenNLP Interpret  https://allennlp.org/interpret
* [todo]
* Captum https://captum.ai/
* Responsible AI Tensorflow https://www.tensorflow.org/resources/responsible-ai

# Related courses
* Interpretability and Explainability in Machine Learning, COMPSCI 282BR, Harvard University: https://interpretable-ml-class.github.io/ (2019)
* CS 294: Fairness in Machine Learning, UC Berkeley (2017): https://fairmlclass.github.io/
* Trustworthy Machine Learning, University of Toronto: https://www.papernot.fr/teaching/f19-trustworthy-ml (2019)
* Fairness, Explainability, and Accountability for ML, ETH Zurich: https://las.inf.ethz.ch/teaching/feaml-s19 (2019)
* Human-Centered Machine Learning, Saarland University: http://courses.mpi-sws.org/hcml-ws18/ (2018)
* Human-centered Machine Learning, University of Colorado Boulder. https://chenhaot.com/courses/hcml/home.html (2018)
* LING 575 — Ethics in NLP: Including Society in Discourse & Design, University of Washington https://ryan.georgi.cc/courses/575-ethics-win-19/ (2019)
* Computational Ethics for NLP, CMU: http://demo.clab.cs.cmu.edu/ethical_nlp/ (2019)
* https://github.com/stanford-policylab/law-order-algo
* Seminar on Ethical and Social Issues in Natural Language Processing  (Stanford 2020) https://docs.google.com/document/d/1zujyrSTiQ-HKQ66oPDiswihe55jUY7pwHpQPwMHTBFI/edit#
